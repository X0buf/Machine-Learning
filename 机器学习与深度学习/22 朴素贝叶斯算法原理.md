## 	分类算法-朴素贝叶斯算法

1. 概率基础
   1. 概率定义为**一件事情发生的可能性**
      1. 扔出一个硬币，结果头像朝上
      2. 某天是晴天
2. 朴素贝叶斯介绍

## 联合概率和条件概率

- 联合概率：包含多个条件，且所有条件同时成立的概率
  - 记作：   P(A,B)
    - P(A,B) = P(A)P(B)
- 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率
  - 记作：P(A|B)
  - 特性：P(A1,A2,B) = P(A1|B)P(A2|B)
- 注意：此条件概率的成立，是由于A1,A2相互独立的结果

## 朴素贝叶斯-贝叶斯公式

- P(C|W) = $\frac{P(W|C)P(C)}{P(W)}$

- 注： w为给定文档的特征值（频数统计，预测文档提供），c为文档类别

- 公式可以理解为：

- P(C|F1,F2,...) = $\frac{P(F1,F2,...|C)P(C)}{P(F1,F2)}$

## 公式分为三部分：

- P(C)：每个文档类别的概率（某文档类别数/总文档数量）
- P(W|C)：给定类别下特征**（被预测文档中出现的词）**的概率
  - 计算方法：P(F1|C) = Ni/N    **(训练文档中去计算)**
  - Ni为该F1词在C类别所有文档中出现的次数
  - N为所属类别C下的文档所有词出现的次数和
- P(F1,F2,...) 预测文档中每个词的概率

**训练集** 统计结果（指定统计词频）：

## 拉普拉斯平滑

- 问题：从例子我们得到娱乐概率为0，这是不合理的，如果词频列表里面很多出现次数都为0，很可能计算结果都为零。
- 解决方法：拉普拉斯平滑系数
- P(F1|C) = $\frac{Ni+a}{N+am}$
- a为指定的系数一般为1，m为训练文档中统计出的特征词个数

## sklearn朴素贝叶斯实现API

- sklearn.naive_bayes.MultinomialNB
- MultinomialNB
  - sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
    - 朴素贝叶斯分类
    - alpha：拉普拉斯平滑系数