# 决策树的结构，本地保存

1. sklearn.tree.export_graphviz() 该函数能够到处DOT格式
   1. tree.export_grephviz(estimator,out_file='tree.dot',feature_names=[","])
2. 工具：(能够将dot文件转换为pdf，png)
   1. 安装graphviz
   2. ubuntu：sudo apt-get install grephviz    Mac：brew install graphviz
3. 运行命令
   1. dot-Tpng tree.dot -o tree.png

# 决策树的优缺点以及改进

- 优点：
  - 简单的理解和解释，树木可视化
  - 需要很少的数据准备，其他技术通常需要数据归一化
- 缺点：
  - 决策树学习者可以创建不能很好的推广数据和过于复杂的树，这被称为过拟合。
- 改进：
  - 减枝cart算法（决策树API当中已经实现，随机森林参数调优有相关介绍）
  - 随机森林
- 注：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多。

# 集成学习方法-随机森林

1. 什么是随机森林
2. 随机森林的过程，优势
   1. 泰坦尼克号乘客生存分类分析

## 集成学习方法

- 集成学习通过建立几个模型组合来解决单一预测问题。它的工作原理是**生成多个分类器/模型**，各自独立地学习和做出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。

## 什么是随机森林

- 定义：在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

- 随机森林建立多个决策树的过程：
  - 单个树的建立过程
    - N个样本，M个特征
      1. 随机在N个样本当中选择一个样本，重复N次 样本有可能重复
      2. 随机在M个特征当中选出m个特征  m取值

- 为什么要随机抽样训练集？
  - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
- 为什么要又放回地抽样？
  - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练除了都是有很大的差异的；而随机森林随后分类取决于多棵树（弱分类器）的投票表决

## 随机森林API

- class sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None)
  - 随机森林分类器
  - **n_estimators：integer，optional (default=10) 森林里的树木数量 120，200，300，500，800，1200**
  - criteria：string，可选（default="gini"）分割特征的测量方法
  - **max_depth：integer或None，可选（默认=无）树的最大深度 5，8，15，25，30**
  - **max_fratures='auto'，每个决策树的最大特征数量**
    - if "auto",then 'max_fratures=sqrt(n_features)'
    - if "sqrt",then 'max_features=sqrt(n_features)' (same as "auto")
    - if "log2",then 'max_features=log2(n_features)'
    - if None, then 'max_features=n_features'
  - bootstrap：boolean，optional（default=True） 是否在构建树时使用放回抽样